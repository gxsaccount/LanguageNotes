## GPU与CPU ##  
<img src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/gpu-devotes-more-transistors-to-data-processing.png />    
绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元.      
GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。    
而CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分。    
所以与CPU擅长逻辑控制和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。    
所以GPU除了图像处理，也越来越多的参与到计算当中来。    

## 适合的情况 ##  
　　（1）计算密集型的程序。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是SSD, 也实在是太慢了。  

　　（2）易于并行的程序。GPU其实是一种SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。  
　　满足以上两点，就可以用GPU做运算了  
## 三个关键抽象 ##  
**线程组**，**共享内存**和**屏障同步**的层次结构.  
这些抽象提供细粒度数据并行和线程并行，嵌套在粗粒度数据并行和任务并行中。  

## GPU的自动伸缩性 ##  
<img src= https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png/>  
GPU是围绕一组流式多处理器(Streaming Multiprocessors SMs)构建的(有关更多细节，请参见<a text=硬件实现 src= https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation/>)。一个多线程程序被分割成独立执行的线程块，因此一个多处理器的GPU比一个少处理器的GPU在更短的时间内自动执行程序。
v_out
