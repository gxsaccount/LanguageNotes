## 计算方法 ##  
**FLOP/s**:每秒浮点运算数（FLoating Point Operations per Second，FLOP/s）  
**例子**:
    
CPU的配置:
  物理内核\*2  
  内核频率:2.5GHz,(每秒运行2.5×10^9个CPU循环)  
  每个循环可处理32 FLOPs（使用AVX & FMA）  
  CPU理论峰值性能为  
  ![](http://latex.codecogs.com/gif.latex?\\2*2.5*10^9\frac{cycles}{second}*32\frac{FLOP}{cycle}=160GFLOP/s)  
  理论峰值无法实现的原因在于，内存访问同样需要时间

## 存储形式 ##  
逻辑上矩阵/图像/张量是多维度的，实际上它们存储在**线性、一维**的计算机内存中。  
大部分现代深度学习库使用**行主序**作为存储顺序,即同一行的连续元素被存储在相邻位置.  
同时也意味着在线性扫描内存时第一个维度的变化速度最慢  
通常4维张量（如CNN中的张量）的存储顺序是NCHW(batch,channel,height,weight)、NHWC等  
[矩阵存储顺序]



## 1.朴素卷积(Naive Convolution) ##  
  
    for filter in (0,num_filters):
       for channel in (0,input_channels):
          for out_h in (0,output_height):  
              for out_w in (0,output_width):
                  for k_h in (0,kernel_height):
                      for k_w in (0,kernel_width):
                          output[filter, channel, out_h, out_h] +=   
                          kernel[filter, channel, k_h, k_w] *    
                          input[channel, out_h + k_h, out_w + k_w]
     
## 矩阵乘法和卷积:im2col ##  
下图展示了一个正常的3x3卷积：  
[3x3卷积]
下图展示的是该卷积运算被实现为矩阵相乘的形式。右侧的矩阵是im2col的结果，它需要从原始图像中复制像素才能得以构建。左侧的矩阵是卷积的权重，它们已经以这种形式储存在内存中。  
[转化为矩阵相乘]  
矩阵乘积直接得出了卷积输出，无需额外「转换」至原始形式  
*出于视觉简洁考虑，此处将每个图像块作为独立的个体进行展示。而在现实中，不同图像块之间通常会有重叠，因而im2col可能导致内存重叠。生成im2col 缓冲（im2col buffer）和过多内存（inflated memory）所花费的时间必须通过GEMM实现的加速来抵消。???*

## 加速矩阵相乘(GEMM) ##  
加速目标:C_{M*N}+=A_{M*K}*B_{K*N}  

**基础矩阵相乘时间**  
    
    for i in 0..M:    
        for j in 0..N:        
            for k in 0..K:            
                C[i, j] += A[i, k] * B[k, j]

