## 计算方法 ##  
**FLOP/s**:每秒浮点运算数（FLoating Point Operations per Second，FLOP/s）  
**例子**:
    
CPU的配置:
  物理内核\*2  
  内核频率:2.5GHz,(每秒运行2.5×10^9个CPU循环)  
  每个循环可处理32 FLOPs（使用AVX & FMA）  
  CPU理论峰值性能为  
  ![](http://latex.codecogs.com/gif.latex?\\2*2.5*10^9\frac{cycles}{second}*32\frac{FLOP}{cycle}=160GFLOP/s)  
  理论峰值无法实现的原因在于，内存访问同样需要时间

## 存储形式 ##  
逻辑上矩阵/图像/张量是多维度的，实际上它们存储在**线性、一维**的计算机内存中。  
大部分现代深度学习库使用**行主序**作为存储顺序,即同一行的连续元素被存储在相邻位置.  
同时也意味着在线性扫描内存时第一个维度的变化速度最慢  
通常4维张量（如CNN中的张量）的存储顺序是NCHW(batch,channel,height,weight)、NHWC等  
[矩阵存储顺序]



## 1.朴素卷积(Naive Convolution) ##  
  
    for filter in (0,num_filters):
       for channel in (0,input_channels):
          for out_h in (0,output_height):  
              for out_w in (0,output_width):
                  for k_h in (0,kernel_height):
                      for k_w in (0,kernel_width):
                          output[filter, channel, out_h, out_h] +=   
                          kernel[filter, channel, k_h, k_w] *    
                          input[channel, out_h + k_h, out_w + k_w]
     
## 矩阵乘法和卷积:im2col ##  
https://sahnimanas.github.io/post/anatomy-of-a-high-performance-convolution/  
下图展示了一个正常的3x3卷积：  
[3x3卷积]
下图展示的是该卷积运算被实现为矩阵相乘的形式。右侧的矩阵是im2col的结果，它需要从原始图像中复制像素才能得以构建。左侧的矩阵是卷积的权重，它们已经以这种形式储存在内存中。  
[转化为矩阵相乘]  
矩阵乘积直接得出了卷积输出，无需额外「转换」至原始形式  
*出于视觉简洁考虑，此处将每个图像块作为独立的个体进行展示。而在现实中，不同图像块之间通常会有重叠，因而im2col可能导致内存重叠。生成im2col 缓冲（im2col buffer）和过多内存（inflated memory）所花费的时间必须通过GEMM实现的加速来抵消。???*

## 矩阵相乘加速(GEMM) ##  
加速目标:C_{M*N}+=A_{M*K}*B_{K*N}  

**基础矩阵相乘时间**  
    
    for i in 0..M:    
        for j in 0..N:        
            for k in 0..K:            
                C[i, j] += A[i, k] * B[k, j]
    
**内存和cpu计算上的加速**  
内存RAM速度慢于cpu的缓存.  
CPU访问内存时会将临近数据加载(locality of reference)  
[加速前]
对于A:  
一旦我们找到 A[i, k]，则它在该行中的下一个元素A[i, k+1]已经被缓存了.  
对于B:  
列的下一个元素并未出现在缓存中，即出现了缓存缺失（cache miss）。这时尽管获取到了数据，CPU也出现了一次停顿。  
获取数据后，缓存同时也被 B 中同一行的其他元素填满。我们实际上并不会使用到它们，因此它们很快就会被删除。多次迭代后，当我们需要那些元素时，我们将再次获取它们。我们在用实际上不需要的值污染缓存。  
**重新修改loop，以充分利用缓存能力。**  
    
    for i in 0..M:    
        for k in 0..K:        
            for j in 0..N:
[加速后]  

**平铺(Tiling)**  
另一个缓存问题:  
对于A中的每一行，我们针对B中所有列进行循环。而对于 B 中的每一步，我们会在缓存中加载一些新的列，去除一些旧的列。当到达A的下一行时，我们仍旧重新从第一列开始循环。我们不断在缓存中添加和删除同样的数据，即缓存颠簸（cache thrashing）。  
如果所有数据均适应缓存，则颠簸不会出现。如果我们处理的是小型矩阵，则它们会舒适地待在缓存里，不用经历重复的驱逐。庆幸的是，我们可以将矩阵相乘分解为子矩阵。要想计算 C 的r×c平铺，我们仅需要A的r行和B的c列。接下来，我们将 C 分解为6x16的平铺：  
    
    C(x, y) += A(k, y) * B(x, k);
    C.update().tile(x, y, xo, yo, xi, yi, 6, 16)
    /*in pseudocode:for xo in 0..N/16:    
    for yo in 0..M/6:        
        for yi in 6:            
            for xi in 0..16:                
                for k in 0..K:                    
                    C(...) = ...*/  
我们将x,y 维度分解为外侧的xo,yo和内侧的xi,yi。我们将为该6x16 block优化micro-kernel（即xi,yi），然后在所有block上运行micro-kernel（通过xo,yo进行迭代）。  
**单指令流多数据流加速**  
向量化 & FMA大部分现代CPU支持SIMD（Single Instruction Multiple Data，单指令流多数据流）。在同一个CPU循环中，SIMD可在多个值上同时执行相同的运算/指令（如加、乘等）。如果我们在4个数据点上同时运行SIMD指令，就会直接实现4倍的加速。  

[SIMD]  


**FMA（Fused Multiply-Add）**  


**多线程**  
对于非常小的规模而言，性能反而下降了。这是因为工作负载很小，线程花费较少的时间来处理工作负载，而花费较多的时间同步其他线程。  



**展开（Unrolling）**  
循环使我们避免重复写同样代码的痛苦，但同时它也引入了一些额外的工作，如检查循环终止、更新循环计数器、指针运算等。如果手动写出重复的循环语句并展开循环，我们就可以减少这一开销。  
展开是几乎完全被编译器负责的另一种优化方式，除了我们想要更多掌控的micro-kernel  

    C.update().tile(x, y, xo, yo, xi, yi, 6, 16).reorder(xi, yi, k, xo, yo).vectorize(xi, 8).unroll(xi).unroll(yi)
    /*in pseudocode:for xo in 0..N/16:    
    for parallel yo:        
        for k in 0..K:            
            C(xi:xi+8, yi+0)            
            C(xi:xi+8, yi+1)            
            ...            
            C(xi:xi+8, yi+5)            
            C(xi+8:xi+16, yi+0)            
            C(xi+8:xi+16, yi+1)            
            ...            
            C(xi+8:xi+16, yi+5)*/
    
    
    



# Winograd #  
https://arxiv.org/abs/1509.09308  
https://hey-yahei.cn/2019/08/21/winograd_convolution/#%E5%B5%8C%E5%A5%97%E5%AE%9E%E7%8E%B0    
更适合小卷积，如3\*3的卷积
**核心是减少矩阵的乘法运算次数**  
例子：
先以1维卷积为例，输入信号为![](https://latex.codecogs.com/gif.latex?d=\left[&space;\begin{array}{llll}{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}&space;&&space;{d_{3}}\end{array}\right]^{T})，卷积核为![](https://latex.codecogs.com/gif.latex?g=\left[&space;\begin{array}{lll}{g_{0}}&space;&&space;{g_{1}}&space;&&space;{g_{2}}\end{array}\right]^{T})，则卷积可写成如下矩阵乘法形式：  
<img src="https://latex.codecogs.com/gif.latex?F(2,3)&space;=&space;\begin{bmatrix}&space;{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}\\&space;{d_{1}}&&space;{d_{2}}&space;&&space;{d_{3}}&space;\end{bmatrix}&space;\begin{bmatrix}&space;{g_{0}}&space;&&space;{g_{1}}&space;&&space;{g_{2}}&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;{r_{0}}&space;&{r_{1}}&space;\end{bmatrix}" title="F(2,3) = \begin{bmatrix} {d_{0}} & {d_{1}} & {d_{2}}\\ {d_{1}}& {d_{2}} & {d_{3}} \end{bmatrix} \begin{bmatrix} {g_{0}} & {g_{1}} & {g_{2}} \end{bmatrix} = \begin{bmatrix} {r_{0}} &{r_{1}} \end{bmatrix}" />  
如果是一般的矩阵乘法，则需要6次乘法和4次加法，如下：  
<img src="https://latex.codecogs.com/gif.latex?\begin{array}{l}{r_{0}=\left(d_{0}&space;\cdot&space;g_{0}\right)&plus;\left(d_{1}&space;\cdot&space;g_{1}\right)&plus;\left(d_{2}&space;\cdot&space;g_{2}\right)}&space;\\&space;{r_{1}=\left(d_{1}&space;\cdot&space;g_{0}\right)&plus;\left(d_{2}&space;\cdot&space;g_{1}\right)&plus;\left(d_{3}&space;\cdot&space;g_{2}\right)}\end{array}" title="\begin{array}{l}{r_{0}=\left(d_{0} \cdot g_{0}\right)+\left(d_{1} \cdot g_{1}\right)+\left(d_{2} \cdot g_{2}\right)} \\ {r_{1}=\left(d_{1} \cdot g_{0}\right)+\left(d_{2} \cdot g_{1}\right)+\left(d_{3} \cdot g_{2}\right)}\end{array}" />   
但是，卷积运算中输入信号转换成的矩阵不是任意矩阵，其中**有规律地分布着大量的重复元素**，比如第1行和第2行的d1和d2，卷积转换成的矩阵乘法比一般矩阵乘法的问题域更小，这就让优化存在了可能:    
Winograd:    
<img src="https://latex.codecogs.com/gif.latex?F(2,3)=\left[&space;\begin{array}{lll}{d_{0}}&space;&&space;{d_{1}}&space;&&space;{d_{2}}&space;\\&space;{d_{1}}&space;&&space;{d_{2}}&space;&&space;{d_{3}}\end{array}\right]&space;\left[&space;\begin{array}{l}{g_{0}}&space;\\&space;{g_{1}}&space;\\&space;{g_{2}}\end{array}\right]=\left[&space;\begin{array}{c}{m_{1}&plus;m_{2}&plus;m_{3}}&space;\\&space;{m_{2}-m_{3}-m_{4}}\end{array}\right]" title="F(2,3)=\left[ \begin{array}{lll}{d_{0}} & {d_{1}} & {d_{2}} \\ {d_{1}} & {d_{2}} & {d_{3}}\end{array}\right] \left[ \begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[ \begin{array}{c}{m_{1}+m_{2}+m_{3}} \\ {m_{2}-m_{3}-m_{4}}\end{array}\right]" />  
其中：   
<img src="https://latex.codecogs.com/gif.latex?\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right)&space;g_{0}}&space;&&space;{m_{2}=\left(d_{1}&plus;d_{2}\right)&space;\frac{g_{0}&plus;g_{1}&plus;g_{2}}{2}}&space;\\&space;{m_{4}=\left(d_{1}-d_{3}\right)&space;g_{2}}&space;&&space;{m_{3}=\left(d_{2}-d_{1}\right)&space;\frac{g_{0}-g_{1}&plus;g_{2}}{2}}\end{array}" title="\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right) g_{0}} & {m_{2}=\left(d_{1}+d_{2}\right) \frac{g_{0}+g_{1}+g_{2}}{2}} \\ {m_{4}=\left(d_{1}-d_{3}\right) g_{2}} & {m_{3}=\left(d_{2}-d_{1}\right) \frac{g_{0}-g_{1}+g_{2}}{2}}\end{array}" />  

在神经网络的推理阶段，卷积核上的元素是固定的，因此**g上的运算可以提前算好**，预测阶段只需计算一次，可以忽略，所以一共所需的运算次数为d与m上的运算次数之和，即4次乘法和8次加法。  
与直接运算的6次乘法和4次加法相比，乘法次数减少，加法次数增加。在计算机中，乘法一般比加法慢，通过减少减法次数，增加少量加法，可以实现加速。  
