## 1.量化定义 ##  
目的:优化离散变量  
(1)矩阵分解  IxJ = (IxR) * (RxJ)  主要为了内存大小占用,使用较少,计算相关问题  
(2)剪枝,将某些参数置为0,对cuda计算无加速, 结构化剪枝(例如将整个chanel去掉,可以加速,减少内存)  
(3) automl,神经网络搜索搜索出一些较小的网络  


## 2.量化挑战##  
正向传播:
1.精度减小,使得模型表达能力不足
2.trade-off:量化范围和精度
3.均匀量化,非均匀量化(效果好,不好实现)  
反向传播:
<img src="https://latex.codecogs.com/gif.latex?h=Q(W_{T}^{f}\times&space;x)" title="h=Q(W_{T}^{f}\times x)" />

<img src="https://latex.codecogs.com/gif.latex?y=P(h)" title="y=P(h)" />
Q为量化函数,f表示float,W为原始参数,x为输入,y为输出

对损失函数L求导:
<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{W_{f}^{T}}=\frac{\partial&space;y}{\partial&space;h}\frac{\partial&space;h}{Q(W_{f}^{T})}\frac{\partial&space;Q(W_{f}^{T})}{W_{f}^{T}}" title="\frac{\partial L}{W_{f}^{T}}=\frac{\partial y}{\partial h}\frac{\partial h}{Q(W_{f}^{T})}\frac{\partial Q(W_{f}^{T})}{W_{f}^{T}}" />

但是Q,P是离散的函数,求导值为0,所以梯度无法到达W   
方法一:强行认为Q,P的导数为1,前向使用量化函数,反向传播则视为无量化函数  
问题:straight-forward Estimator(STE)  
效果还行,网络可以训练,但是loss的走向画出可以发现波动较大,因为此方法相当于让量化后的梯度和量化前的梯度一致,是**gradient mismatch**  
另外需要注意,需要做clip,避免值太大  

量化主要处理三个部分   
1.Pre-Quant Transform,调整参数分布  
2.Projection 将之前连续的值映射到量化的维度上(关键)    
3.Post-Quant Transform 再处理/再参数化(Reparameterrization),例如乘quantization factor.  
   
   Aq = Q_1(A)
   Wq = Q_1(W)
   [Aq*r+b] * [Wq*a] //r和b通过网络训练得到,使用较小的计算量来提升网络表达能力,a是固定的,不可更新  
   = [Aq*Wq] * ar + ab[1*Wq]  //ab[1*Wq]  可以提前算出,复用  
   
   
     
量化resolution和resolution的trade-off   
参数和权重的特点,long-tail 和bell-shape的分布  


 






 KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)
校准方法:  
● 在验证集上做 FP32模型推理.  
● 对于每一层:  
○ 收集 histograms of activations.  
○ generate many quantized distributions with different saturation thresholds.  
○ 选择KL_divergence(ref_distr, quant_distr)最小的阈值.  

候选Q的分布  
1.KL_divergence(P, Q) requires that len(P) == len(Q)   
2.Candidate distribution Q is generated after merging ‘ i ’ bins from bin[0] to bin[i-1] into 128 bins  
Afterwards Q has to be ‘expanded’ again into ‘i’ bins  

Here is a simple example: reference distribution P consisting of 8 bins, we want to quantize into 2 bins:  
P = [ 1, 0, 2, 3, 5, 3, 1, 7]  
we merge into 2 bins (8 / 2 = 4 consecutive bins are merged into one bin)  
[1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16]  
then proportionally expand back to 8 bins, we preserve empty bins from the original distribution P:  
Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4]  
now we should normalize both distributions, after that we can compute KL_divergence  
P /= sum(P)  
Q /= sum(Q)  
result = KL_divergence(P, Q)  
