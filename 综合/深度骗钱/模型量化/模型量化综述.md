## 1.量化定义 ##  
目的:优化离散变量  
(1)矩阵分解  IxJ = (IxR) * (RxJ)  主要为了内存大小占用,使用较少,计算相关问题  
(2)剪枝,将某些参数置为0,对cuda计算无加速, 结构化剪枝(例如将整个chanel去掉,可以加速,减少内存)  
(3) automl,神经网络搜索搜索出一些较小的网络  


## 2.量化挑战##  
正向传播:
1.精度减小,使得模型表达能力不足
2.trade-off:量化范围和精度
3.均匀量化,非均匀量化(效果好,不好实现)  
反向传播:
<img src="https://latex.codecogs.com/gif.latex?h=Q(W_{T}^{f}\times&space;x)" title="h=Q(W_{T}^{f}\times x)" />

<img src="https://latex.codecogs.com/gif.latex?y=P(h)" title="y=P(h)" />
Q为量化函数,f表示float,W为原始参数,x为输入,y为输出

对损失函数L求导:
<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{W_{f}^{T}}=\frac{\partial&space;y}{\partial&space;h}\frac{\partial&space;h}{Q(W_{f}^{T})}\frac{\partial&space;Q(W_{f}^{T})}{W_{f}^{T}}" title="\frac{\partial L}{W_{f}^{T}}=\frac{\partial y}{\partial h}\frac{\partial h}{Q(W_{f}^{T})}\frac{\partial Q(W_{f}^{T})}{W_{f}^{T}}" />

但是Q,P是离散的函数,求导值为0  


 KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)
校准方法:  
● 在验证集上做 FP32模型推理.
● 对于每一层:
○ 收集 histograms of activations.
○ generate many quantized distributions with different saturation thresholds.
○ 选择KL_divergence(ref_distr, quant_distr)最小的阈值.

候选Q的分布  
1.KL_divergence(P, Q) requires that len(P) == len(Q)  
2.Candidate distribution Q is generated after merging ‘ i ’ bins from bin[0] to bin[i-1] into 128 bins
Afterwards Q has to be ‘expanded’ again into ‘i’ bins

Here is a simple example: reference distribution P consisting of 8 bins, we want to quantize into 2 bins:
P = [ 1, 0, 2, 3, 5, 3, 1, 7]
we merge into 2 bins (8 / 2 = 4 consecutive bins are merged into one bin)
[1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16]
then proportionally expand back to 8 bins, we preserve empty bins from the original distribution P:
Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4]
now we should normalize both distributions, after that we can compute KL_divergence
P /= sum(P)
Q /= sum(Q)
result = KL_divergence(P, Q)
